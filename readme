https://github.com/ssbandjl/MLNX_OFED_SRC-5.9-0.5.6.0.git


编译驱动: https://enterprise-support.nvidia.com/s/article/howto-compile-mlnx-ofed-for-different-linux-kernel-distribution--160---160-x
./mlnxofedinstall --add-kernel-support

modinfo mlx4_core | head

# rpmbuild --rebuild --define "_topdir $PWD/kernel" --nodeps --define '_dist %{nil}' --define 'configure_options --with-core-mod --with-user_mad-mod --with-user_access-mod --with-addr_trans-mod --with-mthca-mod --with-mlx4-mod --with-mlx4_en-mod --with-mlx4_vnic-mod --with-mlx5-mod --with-cxgb3-mod --with-cxgb4-mod --with-nes-mod --with-ipoib-mod --with-amso1100-mod --with-sdp-mod --with-srp-mod --with-rds-mod --with-iser-mod --with-e_ipoib-mod --with-nfsrdma-mod --with-9pnet_rdma-mod --with-9p-mod --with-cxgb3i-mod --with-cxgb4i-mod ' --define '_prefix /usr' SRPMS/mlnx-ofa_kernel-*.src.rpm



# /etc/infiniband/info

prefix=/usr

Kernel=2.6.32-573.el6.ppc64

Configure options: --with-core-mod --with-user_mad-mod --with-user_access-mod --with-addr_trans-mod --with-mthca-mod --with-mlx4-mod --with-mlx4_en-mod --with-mlx4_vnic-mod --with-mlx5-mod --with-cxgb3-mod --with-cxgb4-mod --with-nes-mod --with-ipoib-mod --with-amso1100-mod --with-sdp-mod --with-srp-mod --with-rds-mod --with-iser-mod --with-e_ipoib-mod --with-nfsrdma-mod --with-9pnet_rdma-mod --with-9p-mod --with-cxgb3i-mod --with-cxgb4i-mod



install on comm version:
./install.pl <option 1> <option 2> .
./install.pl --help

--bluefield


install ref doc:
https://docs.nvidia.com/networking/display/mlnxofedv561033/installing+mlnx_ofed


version:
MLNX_OFED_SRC-5.9-0.5.6.0


tool:
mlxdevm
SOURCES/mlnx-iproute2-6.0.0/mlxdevm/mlxdevm.c
static void help(void)

main
mlxdevm_init
    mnlu_gen_socket_open
    ifname_map_init
mlxdevm_batch

mlxdevm_cmd
    cmd_port
        MLXDEVM_CMD_PORT_NEW -> .doit = mlxdevm_nl_cmd_port_new_doit

driver:
SOURCES/mlnx-ofed-kernel-5.9/drivers/net/ethernet/mellanox/mlx5/core/sf/devlink.c
mlxdevm_nl_cmd_port_new_doit
    dev->ops->port_new -> mlx5_devm_sf_port_new
        ...
        mlx5_sf_add
        mlx5_sf_table_put
    mlxdevm_port_new_notifiy

struct mlx5_sf *sf


cmd list:
enum mlxdevm_command {
	MLXDEVM_CMD_UNSPEC =	0,

	MLXDEVM_CMD_DEV_GET =	1,		/* can dump */
	MLXDEVM_CMD_DEV_NEW =	3,

	MLXDEVM_CMD_PORT_GET =	5,		/* can dump */
	MLXDEVM_CMD_PORT_SET =	6,
	MLXDEVM_CMD_PORT_NEW =	7,
	MLXDEVM_CMD_PORT_DEL =	8,

	MLXDEVM_CMD_PARAM_GET =	38,		/* can dump */
	MLXDEVM_CMD_PARAM_SET =	39,
	MLXDEVM_CMD_PARAM_NEW =	40,
	MLXDEVM_CMD_PARAM_DEL =	41,

	/* All upstream devlink commands must be added before with the exact
	 * value as that of upstream without fail.
	 * All devm specific must start after MLXDEVM_CMD_EXT_START.
	 * Do not ever change the values. Only add at the end. Never in the
	 * middle.
	 */
	MLXDEVM_CMD_EXT_START = 160,

	MLXDEVM_CMD_EXT_CAP_SET,
	MLXDEVM_CMD_EXT_RATE_NEW,
	MLXDEVM_CMD_EXT_RATE_DEL,
	MLXDEVM_CMD_EXT_RATE_GET,		/* can dump */
	MLXDEVM_CMD_EXT_RATE_SET,

	__MLXDEVM_CMD_MAX,
	MLXDEVM_CMD_MAX = __MLXDEVM_CMD_MAX - 1
};



config tool:
Configuration Using mlxdevm Tool
Create the SF.

SFs are managed using the mlxdevm tool supplied with iproute2 package. The tool is found at /opt/mellanox/iproute2/sbin/mlxdevm.

An SF is created using the mlxdevm tool. The SF is created by adding a port of pcisf flavor.

To create an SF port representor, run:

/opt/mellanox/iproute2/sbin/mlxdevm port add pci/<pci_address> flavour pcisf pfnum <corresponding pfnum> sfnum <sfnum>


rdma:
ibv_post_send
.post_send     = mlx5_post_send,


mlx5_ib_post_send




Nvidia 为 Infiniband 解决和路由用户空间解析服务
这是一个用户空间应用程序，通过 NetLink 与 Linux RDMA 子系统进行交互，并提供 2 种服务：ip2gid（地址解析）和 gid2lid（PathRecord 解析）。


KNEM 是一个 Linux 内核模块，支持大消息的高性能节点内 MPI 通信。 KNEM 适用于自 2.6.15 起的所有 Linux 内核，并支持异步和矢量数据传输以及将内存副本卸载到 Intel I/OAT 硬件。
MPI 实现通常提供基于用户空间双副本的节点内通信策略。 它非常适合小消息延迟，但会浪费许多 CPU 周期、污染缓存并使内存总线饱和。 KNEM 通过 Linux 内核中的单个副本将数据从一个进程传输到另一个进程。 系统调用开销（目前大约为 100 纳秒）对于小消息延迟来说并不好，但拥有单个内存副本对于大消息（通常从几十 KB 开始）来说非常好。

一些特定于供应商的 MPI 堆栈（例如 Myricom MX、Qlogic PSM 等）提供类似的功能，但它们只能在特定的硬件互连上运行，而 KNEM 是通用的（且开源的）。 此外，这些竞争对手都没有像 KNEM 那样提供异步完成模型、I/OAT 复制卸载和/或矢量内存缓冲区支持。


NVIDIA Messaging Accelerator (VMA) 是动态链接的用户空间 Linux 库，用于透明地增强网络密集型应用程序的性能。 它提高了基于消息和流应用程序的性能，例如金融服务市场数据环境和 Web2.0 集群中的应用程序。它允许通过标准套接字 API 编写的应用程序从用户空间通过 Infiniband 和/或以太网运行，并具有完整的网络堆栈旁路。 与在标准以太网或 InfiniBand 互连网络上运行的应用程序相比，延迟减少了 300%，应用程序吞吐量增加了 200%，数据包速率更高，CPU 利用率更高

加速 IO SW 库 (XLIO) 提高了通过标准套接字 API 编写的应用程序的性能，例如 Web 服务、反向代理、缓存、负载平衡、媒体流等。 通过完整的网络堆栈旁路和直接访问加速网络硬件，可以减少延迟、提高吞吐量和有效的 CPU 利用率。XLIO 在运行时与这些应用程序动态链接，重定向标准套接字 API 调用，使它们无需修改即可加速

MSTFLINT 软件包 - 固件刻录和诊断工具1) 概述 该软件包包含适用于 Mellanox 制造的 HCA/NIC 卡的刻录工具和诊断工具。 它还提供对相关源代码的访问。 请参阅文件 LICENSE 了解许可详细信息。 该软件包基于 Mellanox 固件工具 (MFT) 软件包的子集。 有关 MFT 包的完整文档，请参阅 Mellanox 网站的下载页面。


BlueField Rshim 主机驱动程序 rshim 驱动程序提供了一种从外部主机访问 BlueFieldtarget 上的 rshim 资源的方法。 当前版本实现了用于启动映像推送和虚拟控制台访问的设备文件。 它还创建虚拟网络接口来连接到 BlueField 目标，并提供访问内部 rshim 寄存器的方法


init driver:
module_init(ib_uverbs_init);
.write	 = ib_uverbs_write,
	copy_from_user



mlx5_ib_reg_user_mr


debug log:
mlx5_ib_dbg(dev, "start 0x%llx, virt_addr 0x%llx, length 0x%llx, access_flags 0x%x\n",



mlxconfig

SOURCES\mlnx-ofed-kernel-5.9\drivers\net\ethernet\mellanox\mlx5\core\Kconfig
enable sf:
MLX5_ESWITCH
MLX5_SF


mlnxofedinstall
mst start -> which mst -> /usr/bin/mst

mst status

query status of dev:
mlxconfig -d /dev/mst/mt4115_pciconf0 q

mlxconfig -d /dev/mst/mt4115_pciconf0 set SRIOV_EN=1 NUM_OF_VFS=4

lspci -D | grep Mellanox

ibstat

ibdev2netdev

./mlnxofedinstall --enable-sriov --hypervisor

/etc/init.d/openibd restart

locate pci.ids

update-pciids

Mellanox Configuration Registers Access tool


nic=ens785f1
cat /sys/class/net/${nic}/device/sriov_totalvfs
cat /sys/class/infiniband/mlx5_1/device/mlx5_num_vfs
cat /sys/class/net/${nic}/device/sriov_numvfs
cat /sys/class/net/${nic}/device/mlx5_num_vfs

lspci -D | grep Mellanox

echo "ON" > /sys/class/net/ens785f1/device/sriov/0/trust
echo "OFF" > /sys/class/net/ens785f1/device/sriov/0/trust 


tcpdump, ibdump
SOURCES\ibdump-6.0.0\ibdump.c
ibdump -h
usage
--max-burst



SOURCES/ibdump-6.0.0/ibdump.c -> main
(void)hw_sniffer_on;
getopt_long
csv_to_num_array
resources_init
resources_create
    fopen
    ibv_get_device_list
    ibv_alloc_pd
    mtu_enum_to_num
    tmp = (char*)(((u_int64_t)tmp + 0x1000) & ~0xfff)
    mr_flags = IBV_ACCESS_LOCAL_WRITE
    ibv_reg_mr
    qp_init_attr.qp_type = IBV_QPT_UD;
    ibv_create_qp
write_to_file
connect_qp
    modify_qp_to_init
    post_receive
		sge.addr = (uintptr_t)(res->buf[idx])
		rr.sg_list = &sge
		ibv_post_recv
    modify_qp_to_rtr
fw_version_less_than
set_sw_sniffer
	fifth_gen_set_sw_sniffer
		mwrite4(res->mf, 0x23f0, 0xbadc0ffe)
		gcif_set_port_sniffer
	or
	fourth_gen_set_sw_sniffer
		flow_attr.type = IBV_FLOW_ATTR_SNIFFER
		ibv_create_flow
set_pcap_header
	fwrite
while
	poll_completion
		ibv_poll_cq
		 fwrite(hdr, header_size, 1, f)
	post_receive




fifth_gen_set_sw_sniffer
	gcif_set_port_sniffer
	fprintf(stderr, "Failed to set port sniffer1: %s\n", gcif_err_str(rc))

int gcif_set_port_sniffer(mfile* mf, struct connectib_icmd_set_port_sniffer* set_port_sniffer)
{
    SEND_ICMD_FLOW(mf, SET_PORT_SNIFFER, connectib_icmd_set_port_sniffer, set_port_sniffer, 1, 0);
}



rdma statistic


static const struct mlx5_ib_counter basic_q_cnts[] = {
	INIT_Q_COUNTER(rx_write_requests),
	INIT_Q_COUNTER(rx_read_requests),
	INIT_Q_COUNTER(rx_atomic_requests),
	INIT_Q_COUNTER(out_of_buffer),
};


SOURCES\mlnx-iproute2-6.0.0\rdma\stat.c
stat_qp_set_link_auto_sendmsg


mlx5_ib_fill_counters



static const struct ib_device_ops hw_stats_ops = {
	.alloc_hw_port_stats = mlx5_ib_alloc_hw_port_stats,
	.get_hw_stats = mlx5_ib_get_hw_stats,
	.counter_bind_qp = mlx5_ib_counter_bind_qp,
	.counter_unbind_qp = mlx5_ib_counter_unbind_qp,
	.counter_dealloc = mlx5_ib_counter_dealloc,
	.counter_alloc_stats = mlx5_ib_counter_alloc_stats,
	.counter_update_stats = mlx5_ib_counter_update_stats,
	.modify_hw_stat = IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS) ?
			  mlx5_ib_modify_stat : NULL,
};




fill_res_counter_entry
	rdma_counter_query_stats


counter_history_stat_update



RDMA_NLDEV_CMD_STAT_SET




rdma statistic set link

static int rd_cmd(struct rd *rd, int argc, char **argv)
{
	const struct rd_cmd cmds[] = {
		{ NULL,		cmd_help },
		{ "help",	cmd_help },
		{ "dev",	cmd_dev },
		{ "link",	cmd_link },
		{ "resource",	cmd_res },
		{ "system",	cmd_sys },
		{ "statistic",	cmd_stat },


static int stat_set(struct rd *rd)
{
	const struct rd_cmd cmds[] = {
		{ NULL,		stat_help },
		{ "link",	stat_set_link },
		{ "help",	stat_help },
		{ 0 },
	};
	return rd_exec_cmd(rd, cmds, "parameter");
}
stat_set_link
	stat_one_set_link
		stat_one_set_link_opcounters
		RDMA_NLDEV_CMD_STAT_GET
		nldev_stat_get_doit
			stat_get_doit_default_counter
				rdma_counter_get_hwstat_value
					rdma_counter_query_stats
						mlx5_ib_counter_update_stats
							mlx5_ib_query_q_counters


rdma statistic man, 
SOURCES\mlnx-iproute2-6.0.0\man\man8\rdma-statistic.8



mlx5_ib_counters_init
	mlx5_ib_alloc_counters



cap:
--max-burst


WITHOUT_FW_TOOLS


sriov:
mst status

mst start -> /usr/bin/mst
action()
PYTHON_EXEC=`find /usr/bin /bin/ /usr/local/bin -iname 'python*' 2>&1 | grep -e='*python[0-9,.]*' | sort -d | head -n 1`
MST_CONF=/etc/mft/mst.conf
prog="MST (Mellanox Software Tools) driver set"
###########
case "$1" in
    start)
        check_start_args $2
            read -r -a start_flags <<< "${MST_START_FLAGS}"
            for flag in "${start_flags[@]}"
        start $2 $3 -> start()
            check_conf $MST_CONF
            load_module "${MST_PCI_MOD}"     "${MST_PCI_MOD}"     "${modprobe}"  "Loading MST PCI module" $with_unknwon_id
                modprobe
            create_devices "Create devices"  $with_msix $with_unknwon_id
                create_pci_devices
                create_mtusb_devices
                create_ndc_devices



mst status -> 
status)
print_status $2
    is_module
    mdevices_info $verbose
    print_ul_mdevices_info
    ${PYTHON_EXEC} ${MFT_PYTHON_TOOLS}/gearbox/gearbox_status_script.py
    ${PYTHON_EXEC} ${MFT_PYTHON_TOOLS}/mst_retimer/mst_retimer.py status




./mlnxofedinstall --fw-image-dir /tmp/my_fw_bin_files


SOURCES\ofed-scripts-5.9\mlnxofedinstall
--add-kernel-support



rdma link add rxe_eth0 type rxe netdev eth0
SOURCES\mlnx-iproute2-6.0.0\rdma\rdma.c -> main
rd_init(&rd, filename)
	rd_prepare_msg(rd, RDMA_NLDEV_CMD_GET
	rd_send_msg
		mnlu_socket_open(NETLINK_RDMA) -> rdma：允许按需加载 NETLINK_RDMA，提供模块别名，以便如果用户空间打开 RDMA 的 netlink 套接字，则会自动加载内核支持
			mnl_socket_open
		mnl_socket_sendto
	rd_recv_msg(rd, rd_dev_init_cb, rd, seq)
rd_batch
or rd_cmd
int cmd_link(struct rd *rd)
	const struct rd_cmd cmds[] = {
		{ NULL,		link_show },
		{ "add",	link_add },
		{ "delete",	link_del },
		{ "show",	link_show },
		{ "list",	link_show },
		{ "help",	link_help },
		{ 0 }
	};

static int link_add(struct rd *rd)
	link_add_type
		link_add_netdev
			rd_prepare_msg(rd, RDMA_NLDEV_CMD_NEWLINK, &seq,   -> to kernel to kernel 转到内核 rxe_newlink
			mnl_attr_put_strz(rd->nlh, RDMA_NLDEV_ATTR_DEV_NAME, rd->link_name)
			mnl_attr_put_strz(rd->nlh, RDMA_NLDEV_ATTR_LINK_TYPE, rd->link_type)
			mnl_attr_put_strz(rd->nlh, RDMA_NLDEV_ATTR_NDEV_NAME, link_netdev)
			rd_sendrecv_msg(rd, seq)